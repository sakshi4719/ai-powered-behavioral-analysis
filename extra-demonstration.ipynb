{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f274b33d-7f1d-46cf-9ec0-2f6f2416d12a",
   "metadata": {},
   "source": [
    "### Finding subreddit names using Fuzzy Finder\n",
    "Using the example of the phrase \"mental health\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c2c745-43a9-4125-ab9e-5dbffb05e248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for subreddits...\n",
      "Found potential subreddit: mentalhealth (Matched on: 'mental health', Fuzzy Score: 92)\n",
      "Found potential subreddit: MentalHealthSupport (Matched on: 'mental health', Fuzzy Score: 92)\n",
      "Found potential subreddit: MentalHealthUK (Matched on: 'mental health', Fuzzy Score: 92)\n",
      "Found potential subreddit: MentalHealthPH (Matched on: 'mental health', Fuzzy Score: 92)\n",
      "Found potential subreddit: MensMentalHealth (Matched on: 'mental health', Fuzzy Score: 96)\n",
      "Found potential subreddit: MentalHealthPros (Matched on: 'mental health', Fuzzy Score: 92)\n",
      "Found potential subreddit: HolisticMentalHealth (Matched on: 'mental health', Fuzzy Score: 96)\n",
      "Found potential subreddit: BlackMentalHealth (Matched on: 'mental health', Fuzzy Score: 96)\n",
      "Found potential subreddit: MentalHealthIsland (Matched on: 'mental health', Fuzzy Score: 92)\n",
      "Found potential subreddit: MentalHealthHelp (Matched on: 'mental health', Fuzzy Score: 92)\n",
      "Finished searching with keyword: 'mental health'\n",
      "\n",
      "Found Subreddits:\n",
      "mentalhealth\n",
      "MentalHealthPros\n",
      "BlackMentalHealth\n",
      "MentalHealthHelp\n",
      "MentalHealthIsland\n",
      "MentalHealthSupport\n",
      "MentalHealthPH\n",
      "HolisticMentalHealth\n",
      "MensMentalHealth\n",
      "MentalHealthUK\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "user_agent = \"Scraper 1.0 by /u/anonymous20042007\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"QMOPaONJbihjtLRLs4WhTw\",\n",
    "    client_secret=\"JFeDiuNLj_eeBVDn9YrJk9C-JrXefw\",\n",
    "    user_agent = user_agent\n",
    ")\n",
    "base_words = [\"mental health\"]\n",
    "\n",
    "found_subreddits = set()\n",
    "num_subreddits_to_find = 10\n",
    "\n",
    "print(\"Searching for subreddits...\")\n",
    "\n",
    "for word in base_words:\n",
    "    if len(found_subreddits) >= num_subreddits_to_find:\n",
    "        break\n",
    "    try:\n",
    "        for subreddit in reddit.subreddits.search(query=word, limit=None): \n",
    "            if subreddit.display_name.lower() not in [sr.lower() for sr in found_subreddits]:\n",
    "                match = process.extractOne(subreddit.display_name, base_words, scorer=fuzz.partial_ratio)\n",
    "                if match and match[1] >= 70: \n",
    "                    found_subreddits.add(subreddit.display_name)\n",
    "                    print(f\"Found potential subreddit: {subreddit.display_name} (Matched on: '{word}', Fuzzy Score: {match[1]})\")\n",
    "                    if len(found_subreddits) >= num_subreddits_to_find:\n",
    "                        break\n",
    "        print(f\"Finished searching with keyword: '{word}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search with keyword '{word}': {e}\")\n",
    "\n",
    "print(\"\\nFound Subreddits:\")\n",
    "if found_subreddits:\n",
    "    for subreddit_name in list(found_subreddits)[:num_subreddits_to_find]:\n",
    "        print(subreddit_name)\n",
    "else:\n",
    "    print(\"No relevant subreddits found based on the keywords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12de446-2fa8-416f-945e-8d2663dd7e61",
   "metadata": {},
   "source": [
    "### Applying fastText Language Classifier to filter out texts  \n",
    "Texts with a score of less than 0.5 will be filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a8e230-2bd3-4a0e-bbe6-130134be25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    model = fasttext.load_model('lid.176.bin')\n",
    "except ValueError:\n",
    "    print(\"Downloading fastText language identification model...\")\n",
    "    url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open('lid.176.bin', 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    model = fasttext.load_model('lid.176.bin')\n",
    "    print(\"Downloaded and loaded the fastText language identification model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5245f6-19bb-4a4d-b250-452665a0873b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (4928, 14)\n",
      "DataFrame shape after filtering (confidence >= 0.5 OR language == 'en'): (4810, 14)\n",
      "Number of texts removed: 118\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import requests\n",
    "\n",
    "model = fasttext.load_model('lid.176.bin')\n",
    "\n",
    "def classify_language(text):\n",
    "    try:\n",
    "        predictions = model.predict(text, k=1)  \n",
    "        language_code = predictions[0][0].replace('__label__', '')\n",
    "        confidence = predictions[1][0]\n",
    "        return language_code, confidence\n",
    "    except Exception as e:\n",
    "        return None, 0.0\n",
    "\n",
    "df = pd.read_csv(\"processed_reddit_data.csv\")\n",
    "\n",
    "text_column_name = \"proctext\"\n",
    "\n",
    "df[['language', 'language_confidence']] = df[text_column_name].apply(classify_language).apply(pd.Series)\n",
    "\n",
    "english_texts_df = df[\n",
    "    (df['language_confidence'] >= 0.5) | (df['language'] == 'en')\n",
    "].copy()\n",
    "\n",
    "removed_df = df[\n",
    "    ~((df['language_confidence'] >= 0.5) | (df['language'] == 'en'))\n",
    "].copy()\n",
    "\n",
    "removed_count = removed_df.shape[0]\n",
    "\n",
    "print(f\"Original DataFrame shape: {df.shape}\")\n",
    "print(f\"DataFrame shape after filtering (confidence >= 0.5 OR language == 'en'): {english_texts_df.shape}\")\n",
    "print(f\"Number of texts removed: {removed_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad881c53-a609-4e84-8ec7-ffd0266d821d",
   "metadata": {},
   "source": [
    "### Using Named Entity Recognition to find names and contact details\n",
    "If any are found, they will be redacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc8638e4-29e8-41a9-b1c3-d132921f9983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names found: 1102\n",
      "Number of emails found: 12\n",
      "Number of phone numbers found: 6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_personal_info(text):\n",
    "    if isinstance(text, str):\n",
    "        doc = nlp(text)\n",
    "        names = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "        emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "        phones = re.findall(r\"(\\+\\d{1,2}\\s?)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\", text)\n",
    "        return names, emails, phones\n",
    "    else:\n",
    "        return [], [], []\n",
    "\n",
    "df = pd.read_csv(\"data_before_processing.csv\")\n",
    "text_column = \"selftext\"\n",
    "df[['names', 'emails', 'phones']] = df[text_column].apply(extract_personal_info).apply(pd.Series)\n",
    "\n",
    "name_count = df['names'].apply(len).sum()\n",
    "email_count = df['emails'].apply(len).sum()\n",
    "phone_count = df['phones'].apply(len).sum()\n",
    "\n",
    "print(f\"Number of names found: {name_count}\")\n",
    "print(f\"Number of emails found: {email_count}\")\n",
    "print(f\"Number of phone numbers found: {phone_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22962a6-72c0-4cac-9216-45cdb88f7286",
   "metadata": {},
   "source": [
    "### Topic Clustering/Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48064b-f36d-450e-97f2-5c4176792f52",
   "metadata": {},
   "source": [
    "##### Using Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f19186d9-070e-415a-8ec6-718eee65ef1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 16:06:44,677 - top2vec - INFO - Pre-processing documents for training\n",
      "2025-04-05 16:06:46,070 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
      "2025-04-05 16:06:49,189 - top2vec - INFO - Creating joint document/word embedding\n",
      "2025-04-05 16:07:59,177 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2025-04-05 16:08:09,320 - top2vec - INFO - Finding dense areas of documents\n",
      "2025-04-05 16:08:09,500 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "# Top2Vec\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "df = pd.read_csv(\"data_before_processing.csv\")\n",
    "df['selftext'] = df['selftext'].fillna('') \n",
    "docs = df.selftext.tolist()\n",
    "model = Top2Vec(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "273731e8-ec19-4d91-ba47-ae2778b1e7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4744  184]\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(topic_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92bd9947-c0ff-4720-80c5-8312f9733222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(topic_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "285ad79e-3ad3-4b94-9e89-bf2cdfb147ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "753d41d2-c3b5-4664-9a5d-9bd2420172dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "words: ['depression' 'coping' 'depressive' 'depressed' 'anxiety' 'relapse'\n",
      " 'suicidal' 'therapy' 'overcome' 'suicide' 'antidepressants' 'relapsed'\n",
      " 'quitting' 'suffer' 'addiction' 'overwhelmed' 'therapist' 'bipolar'\n",
      " 'recovery' 'psychiatrist' 'miserable' 'suffering' 'addicted' 'ocd'\n",
      " 'anxious' 'dying' 'hopeless' 'advice' 'cope' 'stress' 'survive' 'self'\n",
      " 'cravings' 'psych' 'struggle' 'quit' 'disorder' 'chronic' 'insomnia'\n",
      " 'alone' 'struggling' 'stressed' 'diagnosed' 'heal' 'sober' 'stressful'\n",
      " 'addict' 'medications' 'panic' 'medication']\n",
      "1\n",
      "words: ['so' 'er' 'yeah' 'oh' 'he' 'but' 'and' 'we' 'th' 'for' 'fucking' 'good'\n",
      " 'of' 'she' 'that' 'my' 'its' 'ok' 'son' 've' 'okay' 'they' 'says' 'as'\n",
      " 'im' 'this' 'now' 'their' 'great' 'then' 'it' 'wanna' 'isn' 'hey' 'well'\n",
      " 'right' 'll' 'don' 'you' 'still' 'aren' 'last' 'to' 'the' 'or' 'other'\n",
      " 'man' 'notice' 'after' 'actually']\n"
     ]
    }
   ],
   "source": [
    "for words, scores, num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(num)\n",
    "    print(f\"words: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4358e-3c89-4284-9cc9-3868370fbdc3",
   "metadata": {},
   "source": [
    "##### Using BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02c3648c-1ce1-49b2-9f1c-660b2571be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topic  Count                                      Name  \\\n",
      "0      -1   2196                    -1_feel_like_want_know   \n",
      "1       0    762                   0_addict_drug_smoke_get   \n",
      "2       1    186                      1_hate_life_want_die   \n",
      "3       2    175           2_relationship_friend_feel_love   \n",
      "4       3    172                       3_job_work_get_want   \n",
      "5       4    132                     4_take_anxieti_mg_day   \n",
      "6       5    116                   5_depress_feel_like_get   \n",
      "7       6    113                6_feel_like_feel like_know   \n",
      "8       7    104        7_mental_mental health_help_health   \n",
      "9       8     67          8_heart_chest_symptom_heart rate   \n",
      "10      9     62                   9_friend_feel_talk_like   \n",
      "11     10     42                10_father_dad_parent_would   \n",
      "12     11     41       11_panic_attack_panic attack_breath   \n",
      "13     12     39               12_adhd_depress_get_diagnos   \n",
      "14     13     34               13_sleep_night_anxieti_wake   \n",
      "15     14     28                  14_mom_famili_sister_dad   \n",
      "16     15     27                   15_tire_want_better_get   \n",
      "17     16     26          16_autism_feel_autist_intellectu   \n",
      "18     17     26               17_stomach_eat_anxieti_feel   \n",
      "19     18     25                  18_cut_blade_wrist_bleed   \n",
      "20     19     25        19_repuls_repuls repuls_peopl_like   \n",
      "21     20     24                    20_know_feel_even_like   \n",
      "22     21     23           21_anxieti_pierc_anxiou_pumpkin   \n",
      "23     22     22                  22_motiv_feel_anyth_like   \n",
      "24     23     19                 23_ocd_think_obsess_worri   \n",
      "25     24     19            24_anxieti_symptom_health_feel   \n",
      "26     25     18             25_self harm_harm_self_realli   \n",
      "27     26     17                   26_life_like_peopl_even   \n",
      "28     27     16                     27_like_one_love_know   \n",
      "29     28     15             28_vision_eye_anxieti_symptom   \n",
      "30     29     15            29_weight_eat_gain_gain weight   \n",
      "31     30     14              30_depress_feel_thing_realli   \n",
      "32     31     13                     31_day_today_hope_sad   \n",
      "33     32     13                 32_job_work_mental_health   \n",
      "34     33     13        33_app_uscandid_uscandid com_phone   \n",
      "35     34     13                     34_feel_tri_want_like   \n",
      "36     35     12                 35_day_small_shower_brush   \n",
      "37     36     12                         36_att_jag_och_är   \n",
      "38     37     12  37_scare_breakdown fuck_gurgl_fuck scare   \n",
      "39     38     11                  38_feel_matter_like_head   \n",
      "40     39     11           39_kill_get help_sure go_hotlin   \n",
      "41     40     11     40_messag_script_answer_euphoria high   \n",
      "42     41     10                     41_know_day_late_feel   \n",
      "43     42     10                  42_life_see_talk guy_bcz   \n",
      "44     43     10                  43_feel_weight_head_like   \n",
      "\n",
      "                                       Representation  \\\n",
      "0   [feel, like, want, know, get, go, life, even, ...   \n",
      "1   [addict, drug, smoke, get, time, use, know, da...   \n",
      "2   [hate, life, want, die, wish, fuck, live, woul...   \n",
      "3   [relationship, friend, feel, love, like, life,...   \n",
      "4   [job, work, get, want, life, money, year, go, ...   \n",
      "5   [take, anxieti, mg, day, effect, zoloft, presc...   \n",
      "6   [depress, feel, like, get, want, life, help, y...   \n",
      "7   [feel, like, feel like, know, think, happi, ev...   \n",
      "8   [mental, mental health, help, health, therapis...   \n",
      "9   [heart, chest, symptom, heart rate, rate, anxi...   \n",
      "10  [friend, feel, talk, like, want, know, one, gr...   \n",
      "11  [father, dad, parent, would, mom, know, care, ...   \n",
      "12  [panic, attack, panic attack, breath, fear, al...   \n",
      "13  [adhd, depress, get, diagnos, anxieti, help, t...   \n",
      "14  [sleep, night, anxieti, wake, bed, hour, aslee...   \n",
      "15  [mom, famili, sister, dad, like, never, want, ...   \n",
      "16  [tire, want, better, get, get better, tire wan...   \n",
      "17  [autism, feel, autist, intellectu, peopl, thin...   \n",
      "18  [stomach, eat, anxieti, feel, make, anxiou, pa...   \n",
      "19  [cut, blade, wrist, bleed, razor, slit wrist, ...   \n",
      "20  [repuls, repuls repuls, peopl, like, hate, pos...   \n",
      "21  [know, feel, even, like, year, go, diagnos, ge...   \n",
      "22  [anxieti, pierc, anxiou, pumpkin, drive, bad, ...   \n",
      "23  [motiv, feel, anyth, like, go, know, realli, d...   \n",
      "24  [ocd, think, obsess, worri, feel, diagnos, thi...   \n",
      "25  [anxieti, symptom, health, feel, day, doctor, ...   \n",
      "26  [self harm, harm, self, realli, suicid, want, ...   \n",
      "27  [life, like, peopl, even, feel, live, make, ti...   \n",
      "28  [like, one, love, know, want, would, everi, th...   \n",
      "29  [vision, eye, anxieti, symptom, mri, brain, tu...   \n",
      "30  [weight, eat, gain, gain weight, know, look, f...   \n",
      "31  [depress, feel, thing, realli, like, life, fee...   \n",
      "32  [day, today, hope, sad, coupl month, singl, ti...   \n",
      "33  [job, work, mental, health, posit, mental heal...   \n",
      "34  [app, uscandid, uscandid com, phone, cricket, ...   \n",
      "35  [feel, tri, want, like, go, sad, get, thing, l...   \n",
      "36  [day, small, shower, brush, depress, everi day...   \n",
      "37  [att, jag, och, är, hotlin, ai, chat, help, br...   \n",
      "38  [scare, breakdown fuck, gurgl, fuck scare, men...   \n",
      "39  [feel, matter, like, head, thought, know, left...   \n",
      "40  [kill, get help, sure go, hotlin, knife, diffe...   \n",
      "41  [messag, script, answer, euphoria high, get, e...   \n",
      "42  [know, day, late, feel, today, struggl, depres...   \n",
      "43  [life, see, talk guy, bcz, insur polici, kid, ...   \n",
      "44  [feel, weight, head, like, feel like, disord, ...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [whole stori way fuck long one singl reddit po...  \n",
      "1   [tldr littl brother go path substanc alcohol u...  \n",
      "2   [hate use hate still affect day haunt haunt ac...  \n",
      "3   [hi want say sum thing need get chest first ho...  \n",
      "4   [updat 1 got bed sent bos basic info would nee...  \n",
      "5   [hi everyon deal anxieti around 10 year signif...  \n",
      "6   [recent get anti-depress review told doctor wo...  \n",
      "7   [hello realli want share happen late realli ne...  \n",
      "8   [sick tri mani differ thing help feel wors 23f...  \n",
      "9   [never anxiou person 24/7 minor chest/should p...  \n",
      "10  [tri go beyond friend famili share feel hear b...  \n",
      "11  [long time break everyday due belief parent wa...  \n",
      "12  [hi everyon struggl anxieti panic attack almos...  \n",
      "13  [sorri long post start rambl lol hi 22f look a...  \n",
      "14  [start wonder alon absolut love sleep nap anyt...  \n",
      "15  [friend funniest dude ever could make head exp...  \n",
      "16  [tire belong hurt everyday noth help feel like...  \n",
      "17  [hi everyon hope great day new subreddit recen...  \n",
      "18  [year ago gastroscopi found noth take mani aci...  \n",
      "19  [hey first post today 15f want tri cut thigh ....  \n",
      "20  [guy feel way realli like show real emot like ...  \n",
      "21  [rememb tell 16 year old time turn 21 would ki...  \n",
      "22  [car land topic anxieti first ever time open a...  \n",
      "23  [basic motiv anyth frustrat make plan feel mot...  \n",
      "24  [hello everyon social ocd moral ocd compuls fe...  \n",
      "25  [struggl health anxieti sinc dad almost pas me...  \n",
      "26  [titl say urg self-harm suicid last year use d...  \n",
      "27  [human paradox one hand part natur subject law...  \n",
      "28  [girlfriend dump fuck deal love much eat aliv ...  \n",
      "29  [hello deal absolut worst health anxieti ever ...  \n",
      "30  [hi everyon post know el hope find advic encou...  \n",
      "31  [struggl mental health howev partner struggl s...  \n",
      "32  [feel part bug part part keep aliv definit die...  \n",
      "33  [last year notic stress anxiou past believ rea...  \n",
      "34  [hi app develop creat safe space teenag expres...  \n",
      "35  [recent 18m go kind crisi everyth feel wrong e...  \n",
      "36  [know take care even small stuff feel exhaust ...  \n",
      "37  [need help one turn therapi expens waitlist lo...  \n",
      "38  [think final remov blade razor scare like puss...  \n",
      "39  [hey know turn talk feel like broken record fr...  \n",
      "40  [well look piti someth sure even make tye firs...  \n",
      "41  [month back post creat script help cope exist ...  \n",
      "42  [psych hospit lot time last year job right liv...  \n",
      "43  [26m x alli abus kid suffer depress anxieti si...  \n",
      "44  [reason physic feel minut chang bodi weight ye...  \n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"processed_reddit_data.csv\")\n",
    "df.dropna(subset=['proctext'], inplace=True)\n",
    "docs = df['proctext'].tolist()\n",
    "docs = [doc for doc in docs if len(doc.split()) > 3] \n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "topic_model = BERTopic(embedding_model=embedding_model, n_gram_range=(1, 2), min_topic_size=10)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f6c21c4-de98-4922-b420-ac912117c3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in the Outlier Topic:\n",
      "[('feel', 0.011628962657444803), ('like', 0.01106535872297469), ('want', 0.009689918554817883), ('know', 0.009243674272186912), ('get', 0.009082980626051802), ('go', 0.008760037316339111), ('life', 0.007680984181544208), ('even', 0.0076293154999353355), ('would', 0.007204829020743607), ('thing', 0.0071073999414292)]\n",
      "\n",
      "Words to potentially remove:\n",
      "['feel', 'like', 'want', 'know', 'get', 'go', 'life', 'even', 'would', 'thing']\n"
     ]
    }
   ],
   "source": [
    "topic_1_words = topic_model.get_topic(-1)\n",
    "print(\"Top words in the Outlier Topic:\")\n",
    "print(topic_1_words)\n",
    "\n",
    "words_to_remove_from_topic_1 = [word for word, _ in topic_1_words]\n",
    "print(\"\\nWords to potentially remove:\")\n",
    "print(words_to_remove_from_topic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0c5ae9c-993d-4888-8de7-3d61ba963686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topic  Count                                      Name  \\\n",
      "0      -1   2417                  -1_time_think_realli_tri   \n",
      "1       0    771                   0_addict_drug_smoke_use   \n",
      "2       1    186         1_heart_panic_attack_panic attack   \n",
      "3       2    148                2_take_dose_effect_anxieti   \n",
      "4       3     98           3_relationship_love_talk_realli   \n",
      "5       4     82                4_depress_help_time_realli   \n",
      "6       5     69                   5_friend_talk_peopl_say   \n",
      "7       6     65                      6_mom_tri_dad_famili   \n",
      "8       7     63              7_tire_tire tire_live_anymor   \n",
      "9       8     59                   8_cut_wrist_kill_suicid   \n",
      "10      9     58                   9_father_dad_parent_mom   \n",
      "11     10     58              10_hate_hate hate_peopl_fuck   \n",
      "12     11     52  11_mental health_health_mental_therapist   \n",
      "13     12     49                 12_day_today_tomorrow_die   \n",
      "14     13     41             13_job_work_interview_compani   \n",
      "15     14     37                14_sleep_night_wake_asleep   \n",
      "16     15     30                15_autism_autist_care_also   \n",
      "17     16     28                  16_fuck_love_parent_shit   \n",
      "18     17     27        17_repuls_repuls repuls_peopl_mind   \n",
      "19     18     24          18_anxieti_happen_anxiou_spoiler   \n",
      "20     19     23               19_ocd_think_obsess_diagnos   \n",
      "21     20     23                20_depress_think_day_start   \n",
      "22     21     23         21_relationship_partner_time_love   \n",
      "23     22     23                 22_weight_eat_gain_realli   \n",
      "24     23     22                 23_money_debt_dollar_year   \n",
      "25     24     21                   24_exam_year_bad_design   \n",
      "26     25     21                  25_job_school_studi_work   \n",
      "27     26     21           26_anxieti_symptom_doctor_blood   \n",
      "28     27     20                 27_school_gcse_grade_year   \n",
      "29     28     19         28_thought_think_someth_overthink   \n",
      "30     29     16             29_self harm_harm_self_hospit   \n",
      "31     30     15               30_stomach_pain_eat_anxieti   \n",
      "32     31     14                 31_famili_peopl_tire_love   \n",
      "33     32     13       32_need remov_remov need_remov_need   \n",
      "34     33     13          33_adhd_depress_anyth_atomoxetin   \n",
      "35     34     13     34_birthday_today_peopl_hate birthday   \n",
      "36     35     12                35_dream_sleep_http_startl   \n",
      "37     36     12                   36_friend_make_job_time   \n",
      "38     37     12            37_adhd_adhd med_med_therapist   \n",
      "39     38     12    38_realli_thought_kitti_kitti adventur   \n",
      "40     39     11        39_faith_wish_born fight_lone lone   \n",
      "41     40     10            40_roommat_car_airbnb_tri kill   \n",
      "42     41     10                    41_live_futur_make_die   \n",
      "43     42     10                 42_angri_parent_child_kid   \n",
      "\n",
      "                                       Representation  \\\n",
      "0   [time, think, realli, tri, peopl, one, year, d...   \n",
      "1   [addict, drug, smoke, use, time, day, year, dr...   \n",
      "2   [heart, panic, attack, panic attack, anxieti, ...   \n",
      "3   [take, dose, effect, anxieti, side effect, mg,...   \n",
      "4   [relationship, love, talk, realli, time, frien...   \n",
      "5   [depress, help, time, realli, year, sad, anymo...   \n",
      "6   [friend, talk, peopl, say, group, make, someth...   \n",
      "7   [mom, tri, dad, famili, never, school, got, si...   \n",
      "8   [tire, tire tire, live, anymor, day, die, wish...   \n",
      "9   [cut, wrist, kill, suicid, blade, bleed, blood...   \n",
      "10  [father, dad, parent, mom, tri, hurt, care, ne...   \n",
      "11  [hate, hate hate, peopl, fuck, live, much, hat...   \n",
      "12  [mental health, health, mental, therapist, hel...   \n",
      "13  [day, today, tomorrow, die, keep, wake, think,...   \n",
      "14  [job, work, interview, compani, month, year, m...   \n",
      "15  [sleep, night, wake, asleep, bed, anxieti, fal...   \n",
      "16  [autism, autist, care, also, peopl, think, dis...   \n",
      "17  [fuck, love, parent, shit, dad, live, tire, pe...   \n",
      "18  [repuls, repuls repuls, peopl, mind, one, live...   \n",
      "19  [anxieti, happen, anxiou, spoiler, wake, hormo...   \n",
      "20  [ocd, think, obsess, diagnos, worri, thought, ...   \n",
      "21  [depress, think, day, start, year, realli, wor...   \n",
      "22  [relationship, partner, time, love, sex, toget...   \n",
      "23  [weight, eat, gain, realli, look, gain weight,...   \n",
      "24  [money, debt, dollar, year, month, gambl, abl,...   \n",
      "25  [exam, year, bad, design, studi, work, therapi...   \n",
      "26  [job, school, studi, work, class, graduat, fin...   \n",
      "27  [anxieti, symptom, doctor, blood, health, pain...   \n",
      "28  [school, gcse, grade, year, help, work, realli...   \n",
      "29  [thought, think, someth, overthink, exampl, ha...   \n",
      "30  [self harm, harm, self, hospit, er, suicid, ba...   \n",
      "31  [stomach, pain, eat, anxieti, abdomin, constip...   \n",
      "32  [famili, peopl, tire, love, mani good, make, e...   \n",
      "33  [need remov, remov need, remov, need, group, w...   \n",
      "34  [adhd, depress, anyth, atomoxetin, brain, bett...   \n",
      "35  [birthday, today, peopl, hate birthday, birthd...   \n",
      "36  [dream, sleep, http, startl, asleep, real, cal...   \n",
      "37  [friend, make, job, time, see, back, mental, o...   \n",
      "38  [adhd, adhd med, med, therapist, sure, diagnos...   \n",
      "39  [realli, thought, kitti, kitti adventur, hello...   \n",
      "40  [faith, wish, born fight, lone lone, born, eno...   \n",
      "41  [roommat, car, airbnb, tri kill, ask, school, ...   \n",
      "42  [live, futur, make, die, job, wish, afford, ga...   \n",
      "43  [angri, parent, child, kid, job, dish, good, m...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [use lead pretti good social alot friend use r...  \n",
      "1   [long post liter nowher el put word though non...  \n",
      "2   [still realli anxiou panic attack moment ago a...  \n",
      "3   [hey everyon share success treatment stack put...  \n",
      "4   [friend 31 experienc passiv suicid ideat diagn...  \n",
      "5   [15 diagnos depress sinc middl school abl deal...  \n",
      "6   [first friend constantli isol biggest problem ...  \n",
      "7   [25 year old woman current sleep couch parent ...  \n",
      "8   [tire fight fail see futur ok fade away doubt ...  \n",
      "9   [kill deep help unfortun way abl take thought ...  \n",
      "10  [7 year ago brother start act differ probabl t...  \n",
      "11  [bare anyth hate anymor embarrass sad guilt ha...  \n",
      "12  [tw substanc abus sorri sensit topic need advi...  \n",
      "13  [time read probabl dead week sinc start think ...  \n",
      "14  [job realli earn money everi job reject hard t...  \n",
      "15  [anxieti around sleep whole seem wors older la...  \n",
      "16  [rememb tell 16 year old time turn 21 kick buc...  \n",
      "17  [16 depress long rememb start smoke 14 deal si...  \n",
      "18  [origin post subreddit felt fit post kind gone...  \n",
      "19  [tire wake anxieti fast heart rate race though...  \n",
      "20  [hello cut chase suffer seriou ocd whole momen...  \n",
      "21  [realli need help deal depress titl say need s...  \n",
      "22  [need advic struggl mental health much stem ch...  \n",
      "23  [might think crazi think sometim realli relati...  \n",
      "24  [use throwaway account need talk horribl anxie...  \n",
      "25  [21 live christian household tri final got col...  \n",
      "26  [almost 28 year old struggl cope head toward m...  \n",
      "27  [anxieti first start feb 17 time normal teenag...  \n",
      "28  [school realli stress right teacher help recen...  \n",
      "29  [step download app clariti journal cbt step ne...  \n",
      "30  [talk suicid self harm angri sorri help think ...  \n",
      "31  [year ago gastroscopi found noth take mani aci...  \n",
      "32  [25f lost cousin overdos 16 child father sibl ...  \n",
      "33  [girlfriend dump fuck deal love much eat aliv ...  \n",
      "34  [anyth anymor everi fiber someth wish way turn...  \n",
      "35  [birthday 22 wtf slip bit probabl pas cut hand...  \n",
      "36  [lot dream delusion mystic dream real place fr...  \n",
      "37  [current crisi state mind day post eloqu make ...  \n",
      "38  [idea talk drive insan right place let psychia...  \n",
      "39  [realli afraid leav hous crazi sometim leav pl...  \n",
      "40  [tri km via work shockingli think way could th...  \n",
      "41  [mom hous need head need hit wall stop think 1...  \n",
      "42  [far behind laughabl 21 year old licens job fu...  \n",
      "43  [fiancé lost job month ago live mom famili sig...  \n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_texts_df.dropna(subset=['proctext'], inplace=True)\n",
    "docs = english_texts_df['proctext'].tolist()\n",
    "docs = [doc for doc in docs if len(doc.split()) > 3]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = ['feel', 'like', 'want', 'know', 'get', 'go', 'life', 'even', 'would', 'thing']\n",
    "\n",
    "all_stopwords = stop_words.union(custom_stopwords)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.lower().split()\n",
    "    filtered_words = [word for word in words if word not in all_stopwords and word.isalnum()]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "docs_no_stopwords = [remove_stopwords(doc) for doc in docs]\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "topic_model = BERTopic(embedding_model=embedding_model, n_gram_range=(1, 2), min_topic_size=10)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs_no_stopwords)\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbafeafd-4d07-4db5-8fd3-81ee8d4e13e2",
   "metadata": {},
   "source": [
    "### Collecting outreach posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e68c1e0-3a58-4935-8f3b-a8f91fb09822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in /home/sakshi/.local/lib/python3.10/site-packages (2.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35ad4774-be51-4e76-b4cd-350367698441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "from transformers import pipeline\n",
    "\n",
    "reddit_df = pd.read_csv(\"reddit_comments.csv\")\n",
    "reddit_df.dropna(subset=['body'], inplace=True)\n",
    "\n",
    "outreach_words_llm = [\n",
    "    \"reach out\", \"connect with\", \"support group\", \"talk to someone\", \"help line\",\n",
    "    \"crisis line\", \"community support\", \"peer support\", \"available to chat\",\n",
    "    \"if you need to talk\", \"we are here for you\", \"offering support\",\n",
    "    \"seeking support\", \"you are not alone\",\n",
    "    \"mental health support\", \"get help\", \"find support\", \"join us\",\n",
    "    \"open to talking\", \"need someone to listen\", \"safe space\",\n",
    "    \"helpline number\", \"text line\", \"call us\", \"message me\",\n",
    "    \"online support\", \"local resources\", \"find a therapist\"]\n",
    "\n",
    "def remove_emojis(text):\n",
    "    clean_text = emoji.replace_emoji(text, '')\n",
    "    return clean_text\n",
    "\n",
    "reddit_df['body_no_emoji'] = reddit_df['body'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4733df5c-806e-4499-a86d-52b8df5e64f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Outreach Posts (based on keywords):\n",
      "                                                 body  \\\n",
      "0  Hi everyone, I am recruiting participants for ...   \n",
      "1  \\ni literally feel like my mind cannot underst...   \n",
      "2  if you live in Canada and need help with anxie...   \n",
      "3  We are seeking individuals with depression and...   \n",
      "4  I texted a suicide/crisis hotline because I ha...   \n",
      "\n",
      "                                       body_no_emoji  outreach_confidence  \n",
      "0  Hi everyone, I am recruiting participants for ...                    0  \n",
      "1  \\ni literally feel like my mind cannot underst...                    0  \n",
      "2  if you live in Canada and need help with anxie...                    0  \n",
      "3  We are seeking individuals with depression and...                    0  \n",
      "4  I texted a suicide/crisis hotline because I ha...                    0  \n"
     ]
    }
   ],
   "source": [
    "def identify_potential_outreach_posts(df, text_column='body_no_emoji', outreach_words=outreach_words_llm):\n",
    "    df['outreach_confidence'] = 0\n",
    "    for index, row in df.iterrows():\n",
    "        text = str(row[text_column]).lower()\n",
    "        confidence = 0\n",
    "        for word in outreach_words:\n",
    "            if re.search(r'\\b' + re.escape(word.lower()) + r'\\b', text):\n",
    "                confidence += 1\n",
    "        df.loc[index, 'outreach_confidence'] = confidence\n",
    "    return df\n",
    "\n",
    "potential_outreach_df = identify_potential_outreach_posts(reddit_df)\n",
    "print(\"Potential Outreach Posts (based on keywords):\\n\", potential_outreach_df[['body', 'body_no_emoji', 'outreach_confidence']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ecf7282b-4d92-4d0c-adff-9237b28cba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  body  \\\n",
      "121  I (17) used to see a therapist for about a yea...   \n",
      "201  Hello this is me trying to reach out to create...   \n",
      "497  Starting now and for the next couple of days, ...   \n",
      "695  I know theres something wrong with me, but i d...   \n",
      "700  I haven't posted on Reddit before and I don't ...   \n",
      "\n",
      "                                         body_no_emoji  outreach_confidence  \n",
      "121  I (17) used to see a therapist for about a yea...                    2  \n",
      "201  Hello this is me trying to reach out to create...                    3  \n",
      "497  Starting now and for the next couple of days, ...                    2  \n",
      "695  I know theres something wrong with me, but i d...                    2  \n",
      "700  I haven't posted on Reddit before and I don't ...                    2  \n"
     ]
    }
   ],
   "source": [
    "high_confidence_df = potential_outreach_df[potential_outreach_df['outreach_confidence'] > 1]\n",
    "print(high_confidence_df[['body', 'body_no_emoji', 'outreach_confidence']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283656c-e26e-4f1e-a761-a3faf92a0047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
