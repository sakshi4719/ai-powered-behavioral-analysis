{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adcc7de1-580d-4fa2-a97f-476f3ec72dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 11.3MB/s]\n",
      "2025-03-24 09:54:12 INFO: Downloaded file to /home/sakshi/stanza_resources/resources.json\n",
      "2025-03-24 09:54:12 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-03-24 09:54:12 INFO: File exists: /home/sakshi/stanza_resources/en/default.zip\n",
      "2025-03-24 09:54:16 INFO: Finished downloading models and saved to /home/sakshi/stanza_resources\n",
      "2025-03-24 09:54:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 11.6MB/s]\n",
      "2025-03-24 09:54:17 INFO: Downloaded file to /home/sakshi/stanza_resources/resources.json\n",
      "2025-03-24 09:54:17 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-03-24 09:54:17 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2025-03-24 09:54:17 INFO: Using device: cuda\n",
      "2025-03-24 09:54:17 INFO: Loading: tokenize\n",
      "2025-03-24 09:54:17 INFO: Loading: mwt\n",
      "2025-03-24 09:54:17 INFO: Loading: ner\n",
      "2025-03-24 09:54:22 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              selftext  \\\n",
      "28   fifty-something father of two, one in college ...   \n",
      "112  i am sure im sick, ive been in the hospital an...   \n",
      "129  iâ€™m a 25 year old woman. i currently sleep on ...   \n",
      "202  i am 16 years old and living in india, and i'm...   \n",
      "248  info before reading this: i am 18, diagnosed w...   \n",
      "\n",
      "                    place_name  \n",
      "28                   [florida]  \n",
      "112                   [canada]  \n",
      "129  [florida, south carolina]  \n",
      "202             [india, india]  \n",
      "248                  [germany]  \n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download('en')\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner', use_gpu=True)\n",
    "\n",
    "def extract_locations(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    locations = [ent.text for ent in doc.ents if ent.type == \"GPE\"]  # Extract locations\n",
    "    return locations if locations else None\n",
    "\n",
    "df['place_name'] = df['selftext'].apply(extract_locations)\n",
    "\n",
    "print(df[['selftext', 'place_name']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a46690a-1969-414f-bab9-85bd79163507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap saved as crisis_heatmap.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "\n",
    "df = pd.read_csv(\"ner_extracted_locations.csv\")\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"crisis_heatmap\")\n",
    "\n",
    "def get_lat_lon(place):\n",
    "    try:\n",
    "        location = geolocator.geocode(place, timeout=10)\n",
    "        if location:\n",
    "            return (location.latitude, location.longitude)\n",
    "    except GeocoderTimedOut:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "df = df.dropna(subset=['place_name'])\n",
    "\n",
    "df['coordinates'] = df['place_name'].apply(get_lat_lon)\n",
    "\n",
    "df = df.dropna(subset=['coordinates'])\n",
    "\n",
    "df[['latitude', 'longitude']] = pd.DataFrame(df['coordinates'].tolist(), index=df.index)\n",
    "\n",
    "m = folium.Map(location=[20, 0], zoom_start=2)  \n",
    "\n",
    "heat_data = df[['latitude', 'longitude']].values.tolist()\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "m.save(\"crisis_heatmap.html\")\n",
    "print(\"Heatmap saved as crisis_heatmap.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37184df6-35c1-4fd6-ae23-f52583a1961e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"['florida']\" \"['canada']\" \"['florida', 'south carolina']\"\n",
      " \"['india', 'india']\" \"['germany']\" \"['japan']\" \"['india']\"\n",
      " \"['somalia', 'somalia']\" \"['london']\" \"['california']\"\n",
      " \"['ohio', 'indiana']\" \"['america']\" \"['ukraine']\"\n",
      " \"['australia', 'canada']\" \"['ontario', 'canada']\" \"['vietnam']\"\n",
      " \"['texas']\" \"['illinois', 'arkansas']\" \"['australia']\" \"['u.s.']\"\n",
      " \"['venezuela']\" \"['augusta']\" \"['philadelphia']\" \"['morgantown']\"\n",
      " \"['texas', 'colorado']\" \"['brooklyn', 'california']\" \"['nyc']\"\n",
      " \"['tennessee']\" \"['sweden']\" \"['tijuana']\" \"['ontario canada']\"\n",
      " \"['braunschweig']\" \"['netherlands']\" \"['afghanistan']\" \"['sarasota']\"\n",
      " \"['ukraine', 'ukraine']\" \"['texas', 'georgia']\" \"['ohio']\" \"['russia']\"\n",
      " \"['colorado']\" \"['mexico']\" \"['mumbai']\" \"['switzerland']\" \"['kerala']\"\n",
      " \"['pakistan']\" \"['iraq', 'canada']\" \"['bolivia']\" \"['indiana']\"\n",
      " \"['malaysia', 'malaysia', 'malaysia', 'malaysia']\"\n",
      " \"['georgia', 'georgia']\" \"['bangalore']\" \"['uzbekistan']\" \"['georgia']\"]\n"
     ]
    }
   ],
   "source": [
    "unique_places = df['place_name'].dropna().unique()\n",
    "print(unique_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219b0cb-0451-4554-be04-4240db9e4e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4daded2-13af-4992-85a4-dc30d7c5e1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
